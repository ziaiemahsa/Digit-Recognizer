{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNiFzmQHXM+HyGJcZ7dKc9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziaiemahsa/Digit-Recognizer/blob/main/NN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXQEFHeCwQHx"
      },
      "source": [
        "#<font color=SteelBlue><b>Building a Neural Network from Scratch using Numpy and Math Libraries for the MNIST dataset of handwritten digits </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####<b>Code Description:</b>\n",
        "Implemented a simple neural network and trained it on the MNIST dataset.\n",
        "\n",
        "Input layer 784 units ->  hidden layer 10 units with ReLU activation -> output layer 10 units for ten digit classes with softmax activation.\n",
        "\n",
        "softmax:\n",
        "1. The naive softmax involves calculating the exponential of each element in the input array Z, followed by dividing each element by the sum of all exponentials. This implementation of the softmax function is straightforward but can result in numerical instability. Overflow errors can occur if Z contains very large elements due to the rapid growth of the exponential function, potentially exceeding the representable range of the data type. Similarly, underflow errors may arise if all elements in Z are very small, causing the sum to approach zero and leading to a loss of precision when dividing by a small number.\n",
        "\n",
        "2. In the improved stability softmax approach, the maximum value in Z is subtracted from each element before computing the exponentials. This adjustment is made to enhance numerical stability by preventing overflow errors that can occur when exponentiating large numbers, potentially leading to inaccurate results. By ensuring that the largest exponent becomes zero, the relative probabilities remain unchanged while improving numerical stability. This method is preferable in practice, particularly when dealing with large numbers in Z, as it mitigates overflow errors and enhances the reliability of the results obtained through softmax calculation."
      ],
      "metadata": {
        "id": "_ZIksQ6VwQHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset:\n",
        "- https://www.kaggle.com/competitions/digit-recognizer/data?select=train.csv\n"
      ],
      "metadata": {
        "id": "atUYIO5xwQHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downlload the data from drive\n",
        "! gdown --id 1nGtuOv94-z00oduoEbXmZOXs-OR0G8zc\n",
        "# unzip the data\n",
        "! unzip digit-recognizer.zip\n",
        "# detele unnecessary files\n",
        "!rm -rf __MACOSX\n",
        "# path to the data\n",
        "data_path = '/content/digit-recognizer/train.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liBXiCOeb6e0",
        "outputId": "9848b1aa-3c31-4ea6-ffd6-80eb450470d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nGtuOv94-z00oduoEbXmZOXs-OR0G8zc\n",
            "To: /content/digit-recognizer.zip\n",
            "100% 15.6M/15.6M [00:00<00:00, 165MB/s]\n",
            "Archive:  digit-recognizer.zip\n",
            "   creating: digit-recognizer/\n",
            "  inflating: __MACOSX/._digit-recognizer  \n",
            "  inflating: digit-recognizer/test.csv  \n",
            "  inflating: __MACOSX/digit-recognizer/._test.csv  \n",
            "  inflating: digit-recognizer/train.csv  \n",
            "  inflating: __MACOSX/digit-recognizer/._train.csv  \n",
            "  inflating: digit-recognizer/sample_submission.csv  \n",
            "  inflating: __MACOSX/digit-recognizer/._sample_submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=black><b>NN for MINST with NumPy and Math</b></font>"
      ],
      "metadata": {
        "id": "11DI2K14wQHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing the necessary libraries and dependencies\n",
        "import numpy as np  # NumPy for numerical operations\n",
        "import pandas as pd  # Pandas for data manipulation\n",
        "import matplotlib.pyplot as plt  # Matplotlib for plotting"
      ],
      "metadata": {
        "id": "6S0Sq5qowQHy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "IMAGE_SIZE = 28 * 28  # Size of each input image (28x28 pixels)\n",
        "NUM_CLASSES = 10  # Number of classes (digits 0-9)\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Load and preprocess the data.\"\"\"\n",
        "    data = pd.read_csv(data_path).values  # Load data from CSV file\n",
        "    np.random.shuffle(data)  # Shuffle the data to avoid any bias\n",
        "    X = data[:, 1:] / 255.0  # Extract pixel values and normalize them\n",
        "    Y = data[:, 0]  # Extract labels\n",
        "    return X.T, Y  # Transpose X to have features in columns\n",
        "\n",
        "def initialize_parameters(hidden_units):\n",
        "    \"\"\"Initialize weights and biases for the neural network.\"\"\"\n",
        "    # Initialize weights and biases for the hidden layer and output layer\n",
        "    W1 = np.random.randn(hidden_units, IMAGE_SIZE) * 0.01  # Weight matrix for hidden layer\n",
        "    b1 = np.zeros((hidden_units, 1))  # Bias vector for hidden layer\n",
        "    W2 = np.random.randn(NUM_CLASSES, hidden_units) * 0.01  # Weight matrix for output layer\n",
        "    b2 = np.zeros((NUM_CLASSES, 1))  # Bias vector for output layer\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(Z, 0)  # ReLU activation function\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"Softmax activation function.\"\"\"\n",
        "    ##  1. Naive Softmax:\n",
        "    #A = np.exp(Z) / sum(np.exp(Z))\n",
        "    #return A\n",
        "    ##  2. Softmax with Improved Stability\n",
        "    exp_Z = np.exp(Z - np.max(Z))  # Subtracting maximum for numerical stability\n",
        "    return exp_Z / np.sum(exp_Z, axis=0)  # Return softmax probabilities\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    \"\"\"Forward propagation through the neural network.\"\"\"\n",
        "    # Perform forward propagation through the network layers\n",
        "    Z1 = W1.dot(X) + b1  # Compute activation of the hidden layer\n",
        "    A1 = ReLU(Z1)  # Apply ReLU activation function\n",
        "    Z2 = W2.dot(A1) + b2  # Compute activation of the output layer\n",
        "    A2 = softmax(Z2)  # Apply softmax activation function\n",
        "    return Z1, A1, Z2, A2  # Return activations for each layer\n",
        "\n",
        "def compute_cost(A2, Y):\n",
        "    \"\"\"Compute the cross-entropy cost.\"\"\"\n",
        "    m = Y.shape[0]  # Number of training examples\n",
        "    # Compute cross-entropy cost\n",
        "    log_probs = -np.log(A2[Y, np.arange(m)])  # Log probabilities of predicted classes\n",
        "    return np.sum(log_probs) / m  # Average cross-entropy cost over all examples\n",
        "\n",
        "def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):\n",
        "    \"\"\"Backward propagation to compute gradients.\"\"\"\n",
        "    m = Y.shape[0]  # Number of training examples\n",
        "    one_hot_Y = np.zeros((NUM_CLASSES, m))  # Initialize one-hot encoded labels\n",
        "    one_hot_Y[Y, np.arange(m)] = 1  # Set true labels to 1 in one-hot encoding\n",
        "\n",
        "    # Compute gradients using backpropagation\n",
        "    dZ2 = A2 - one_hot_Y  # Compute gradient of the output layer\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)  # Gradient of weights for output layer\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)  # Gradient of biases for output layer\n",
        "    dZ1 = np.dot(W2.T, dZ2) * (Z1 > 0)  # Compute gradient of the hidden layer\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)  # Gradient of weights for hidden layer\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)  # Gradient of biases for hidden layer\n",
        "    return dW1, db1, dW2, db2  # Return gradients\n",
        "\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    \"\"\"Update parameters using gradient descent.\"\"\"\n",
        "    # Update parameters using gradient descent\n",
        "    W1 -= learning_rate * dW1  # Update weights for hidden layer\n",
        "    b1 -= learning_rate * db1  # Update biases for hidden layer\n",
        "    W2 -= learning_rate * dW2  # Update weights for output layer\n",
        "    b2 -= learning_rate * db2  # Update biases for output layer\n",
        "    return W1, b1, W2, b2  # Return updated parameters\n",
        "\n",
        "def train_model(X, Y, hidden_units=10, learning_rate=0.01, iterations=1000):\n",
        "    \"\"\"Train the neural network model.\"\"\"\n",
        "    # Initialize parameters for the neural network\n",
        "    W1, b1, W2, b2 = initialize_parameters(hidden_units)\n",
        "    for i in range(iterations):\n",
        "        # Perform forward propagation\n",
        "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "        # Compute cost\n",
        "        cost = compute_cost(A2, Y)\n",
        "        # Perform backward propagation\n",
        "        dW1, db1, dW2, db2 = backward_propagation(X, Y, Z1, A1, Z2, A2, W2)\n",
        "        # Update parameters using gradient descent\n",
        "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
        "        # Print cost every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(\"Iteration {}, Cost: {:.4f}\".format(i, cost))\n",
        "    return W1, b1, W2, b2  # Return trained parameters\n",
        "\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    \"\"\"Make predictions using the trained model.\"\"\"\n",
        "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)  # Perform forward propagation\n",
        "    return np.argmax(A2, axis=0)  # Return indices of maximum probabilities\n",
        "\n",
        "def visualize_prediction(index, X, Y, W1, b1, W2, b2):\n",
        "    \"\"\"Visualize a single prediction.\"\"\"\n",
        "    # Make prediction for a single example\n",
        "    prediction = predict(X[:, index:index+1], W1, b1, W2, b2)[0]\n",
        "    label = Y[index]  # True label of the example\n",
        "    # Display the image and prediction\n",
        "    plt.imshow(X[:, index].reshape(28, 28), cmap='gray')  # Reshape and display image\n",
        "    plt.title(\"Prediction: {}, Label: {}\".format(prediction, label))  # Display prediction and label\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Load and preprocess the data\n",
        "X_train, Y_train = load_data(data_path)\n",
        "\n",
        "# Train the model\n",
        "W1, b1, W2, b2 = train_model(X_train, Y_train, hidden_units=10, learning_rate=0.1, iterations=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3fF4IrSnFG8",
        "outputId": "5d84066e-b1f8-4de6-fbe6-92f4302e4340"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 2.3026\n",
            "Iteration 100, Cost: 1.5890\n",
            "Iteration 200, Cost: 0.6721\n",
            "Iteration 300, Cost: 0.5043\n",
            "Iteration 400, Cost: 0.4288\n",
            "Iteration 500, Cost: 0.3872\n",
            "Iteration 600, Cost: 0.3599\n",
            "Iteration 700, Cost: 0.3402\n",
            "Iteration 800, Cost: 0.3252\n",
            "Iteration 900, Cost: 0.3134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user to select an image to Visualize a single prediction\n",
        "image_index = int(input(\"Enter the index of the image to visualize (0-59999): \"))\n",
        "visualize_prediction(image_index, X_train, Y_train, W1, b1, W2, b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "PMomG93MaaUd",
        "outputId": "d7afa0f8-89cb-4578-870b-0328be0b61ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the index of the image to visualize (0-59999): 76\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm/0lEQVR4nO3dfXRU9Z3H8U8IyQAhmQjkUQIEEGxB8EAlZXkoLllC8AnFVlB3g+sCSkCRtdp4Kg+WYxRtF0Ua3P4B1YJaeho5RYsFJKFowAVDWVSykAYCCwnImhlIyAPkt39wGB2SAHeY5JeE9+ucew5z7/3e+82dy3xyH3InxBhjBABAC+tguwEAwPWJAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAEKr1KdPH02fPt33Oi8vTyEhIcrLywvaOkJCQrRo0aKgLa+9W716tUJCQrRr166gLXPRokUKCQkJ2vLQthBAaODiB83FoVOnThowYIDmzJmj8vJy2+058uGHH7bJkKmrq9P3v/99hYSE6NVXXw14OdOnT1fXrl2D2FnrVVFRodjYWIWEhOgPf/iD7XZwFTrabgCt1wsvvKDk5GRVV1dr+/btysnJ0Ycffqh9+/apS5cuLdrL2LFjdfbsWYWHhzuq+/DDD7VixYpGQ+js2bPq2LF1/hdYvny5SktLbbfRpixYsEBVVVW224ADHAGhSenp6Xr44Yf1b//2b1q9erXmzZunkpISrV+/vsmaysrKZumlQ4cO6tSpkzp0CN4u26lTp1YZQCdOnNALL7ygZ5991nYrbca+ffuUk5PDNmtjCCBctX/8x3+UJJWUlEj69vROcXGxJk2apMjISD300EOSpPr6ei1btkyDBg1Sp06dFBcXp1mzZumbb77xW6YxRkuWLFHPnj3VpUsX3X777friiy8arLupa0A7d+7UpEmTdMMNNygiIkJDhgzRa6+95utvxYoVkuR3SvGixq4BFRYWKj09XVFRUeratavGjx+vHTt2+M1z8RTlJ598ovnz5ysmJkYRERG69957dfLkSb95PR6P9u/fL4/HczWbWJL0s5/9TAMHDtTDDz981TXX4vDhw5o9e7YGDhyozp07q3v37vrxj3+sQ4cONTp/VVWVZs2ape7duysqKkr/8i//0uB9laQ///nPGjNmjCIiIhQZGak77rij0ff2Ul9//bX279/v6GjmySef1L333qsxY8ZcdQ3sa32//qHVKi4uliR1797dN+7cuXNKS0vT6NGj9eqrr/pOzc2aNUurV6/WI488oieeeEIlJSV64403VFhYqE8++URhYWGSLpw2WbJkiSZNmqRJkybp888/14QJE1RbW3vFfjZt2qQ777xTCQkJevLJJxUfH6+vvvpKGzZs0JNPPqlZs2bp2LFj2rRpk95+++0rLu+LL77QmDFjFBUVpWeeeUZhYWF68803NW7cOOXn5yslJcVv/rlz5+qGG27QwoULdejQIS1btkxz5szRe++955snNzdXjzzyiFatWuV3U0VTPvvsM/32t7/V9u3bW+zi/H/913/p008/1dSpU9WzZ08dOnRIOTk5GjdunL788ssGp1vnzJmj6OhoLVq0SEVFRcrJydHhw4d9vyRI0ttvv62MjAylpaXp5ZdfVlVVlXJycjR69GgVFhaqT58+TfbzxhtvaPHixdq6davGjRt3xf7XrVunTz/9VF999VWToYlWygCXWLVqlZFkNm/ebE6ePGmOHDli3n33XdO9e3fTuXNnc/ToUWOMMRkZGUaS+dnPfuZX/9e//tVIMmvWrPEbv3HjRr/xJ06cMOHh4eaOO+4w9fX1vvmee+45I8lkZGT4xm3dutVIMlu3bjXGGHPu3DmTnJxsevfubb755hu/9Xx3WZmZmaap3VySWbhwoe/15MmTTXh4uCkuLvaNO3bsmImMjDRjx45tsH1SU1P91vXUU0+Z0NBQU1FR0WDeVatWNdrDpX2PGDHCTJs2zRhjTElJiZFkXnnllSvWNiUjI8NERERcdp6qqqoG4woKCowk89Zbb/nGXfxZhg8fbmpra33jly5daiSZ9evXG2OMOX36tImOjjYzZszwW2ZZWZlxu91+4xcuXNjg/bk47uJ7faXee/XqZbKysowx3+4n69atu2It7OMUHJqUmpqqmJgYJSUlaerUqeratatyc3N14403+s33+OOP+71et26d3G63/umf/klff/21bxg+fLi6du2qrVu3SpI2b96s2tpazZ071++3/Xnz5l2xt8LCQpWUlGjevHmKjo72mxbIkcP58+f1l7/8RZMnT1bfvn194xMSEvTggw9q+/bt8nq9fjUzZ870W9eYMWN0/vx5HT582Ddu+vTpMsZc1dHP6tWr9d///d96+eWXHfd/LTp37uz7d11dnU6dOqX+/fsrOjpan3/+eYP5Z86c6TuClS68/x07dtSHH34o6cKRaUVFhaZNm+b3/oeGhiolJcX3/jdl0aJFMsZc1dHPSy+9pLq6Oj333HNX+dOiNeEUHJq0YsUKDRgwQB07dlRcXJwGDhzY4CaAjh07qmfPnn7jDhw4II/Ho9jY2EaXe+LECUnyfVDfdNNNftNjYmJ0ww03XLa3i6cDBw8efPU/0GWcPHlSVVVVGjhwYINp3/ve91RfX68jR45o0KBBvvG9evXym+9iz41dD7kSr9errKws/fSnP1VSUpLj+mtx9uxZZWdna9WqVfrf//1fme98SXJj164ufb+6du2qhIQE3+mvAwcOSPr2muGloqKigtL3oUOH9Morr2jFihXXza3m7Q0BhCaNGDFCP/jBDy47j8vlahBK9fX1io2N1Zo1axqtiYmJCVqPNoWGhjY63gTwLfevvvqqamtr9cADD/g+yI8ePSrpQqAdOnRIiYmJjm9Dvxpz587VqlWrNG/ePI0cOVJut1shISGaOnWq6uvrHS/vYs3bb7+t+Pj4BtODdefhggULdOONN2rcuHG+bVZWVibpwi8Uhw4dUq9evYJ65ySCiwBC0PXr10+bN2/WqFGj/E7vXKp3796SLvzG/N3TXidPnrziUUS/fv0kXbj9NjU1tcn5rvZ0XExMjLp06aKioqIG0/bv368OHTo065FJaWmpvvnmG78jrItefPFFvfjiiyosLNStt94a9HX/4Q9/UEZGhn75y1/6xlVXV6uioqLR+Q8cOKDbb7/d9/rMmTM6fvy4Jk2aJOnb9yY2Nvay7821Ki0t1cGDB/32nYtmz54t6UJ4X3qKFq0Hvxog6H7yk5/o/Pnz+sUvftFg2rlz53wfbKmpqQoLC9Py5cv9jhqWLVt2xXUMGzZMycnJWrZsWYMPyu8uKyIiQpKa/DC9KDQ0VBMmTND69ev97qQqLy/X2rVrNXr06IBOHV3tbdhPPPGEcnNz/YY333xT0oXrSLm5uUpOTna8/qsRGhra4Kht+fLlOn/+fKPz/+d//qfq6up8r3NycnTu3Dmlp6dLktLS0hQVFaUXX3zRb76LLr1V/VJXexv2kiVLGmyzi/vcM888o9zcXN/7j9aJIyAE3Y9+9CPNmjVL2dnZ2rNnjyZMmKCwsDAdOHBA69at02uvvab7779fMTExevrpp5Wdna0777xTkyZNUmFhof785z+rR48el11Hhw4dlJOTo7vuuku33nqrHnnkESUkJGj//v364osv9NFHH0mShg8fLunCB3xaWppCQ0M1derURpe5ZMkSbdq0SaNHj9bs2bPVsWNHvfnmm6qpqdHSpUsD2hZXexv2sGHDNGzYML9xF4Nw0KBBmjx5st+0i7cxX81tx3V1dVqyZEmD8d26ddPs2bN155136u2335bb7db3v/99FRQUaPPmzX63239XbW2txo8fr5/85CcqKirSr3/9a40ePVp33323pAvXeHJycvTP//zPGjZsmKZOnaqYmBiVlpbqgw8+0KhRo/TGG2802e/V3oY9evToBuMuHu3cdtttDbYZWh8CCM1i5cqVGj58uN58800999xz6tixo/r06aOHH35Yo0aN8s23ZMkSderUSStXrtTWrVuVkpKiv/zlL7rjjjuuuI60tDRt3bpVixcv1i9/+UvV19erX79+mjFjhm+e++67T3PnztW7776r3/3udzLGNBlAgwYN0l//+ldlZWUpOztb9fX1SklJ0e9+97sGfwNkW2Vlpfr3739V89bW1ur5559vML5fv36aPXu2XnvtNYWGhmrNmjWqrq7WqFGjtHnzZqWlpTW6vDfeeENr1qzRggULVFdXp2nTpun111/3O9354IMPKjExUS+99JJeeeUV1dTU6MYbb9SYMWP0yCOPBPZDo90JMYFcMQVgzZdffqlBgwZpw4YNVxXUQGvFNSCgjdm6datGjhxJ+KDN4wgIAGAFR0AAACsIIACAFQQQAMAKAggAYEWr+zug+vp6HTt2TJGRkS32fSgAgOAxxuj06dNKTEy87LP4Wl0AHTt2rMWfBgwACL4jR440eFr+d7W6U3CRkZG2WwAABMGVPs+bLYBWrFihPn36qFOnTkpJSdFnn312VXWcdgOA9uFKn+fNEkDvvfee5s+fr4ULF+rzzz/X0KFDlZaW5vsiMgAA1OgXdV+jESNGmMzMTN/r8+fPm8TERJOdnX3FWo/HYyQxMDAwMLTxwePxXPbzPuhHQLW1tdq9e7ffF1F16NBBqampKigoaDB/TU2NvF6v3wAAaP+CHkBff/21zp8/r7i4OL/xcXFxvq/L/a7s7Gy53W7fwB1wAHB9sH4XXFZWljwej284cuSI7ZYAAC0g6H8H1KNHD4WGhqq8vNxvfHl5ueLj4xvM73K55HK5gt0GAKCVC/oRUHh4uIYPH64tW7b4xtXX12vLli0aOXJksFcHAGijmuVJCPPnz1dGRoZ+8IMfaMSIEVq2bJkqKyv5Kl4AgE+zBNADDzygkydPasGCBSorK9Ott96qjRs3NrgxAQBw/Wp134jq9XrldrtttwEAuEYej0dRUVFNTrd+FxwA4PpEAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYEXQA2jRokUKCQnxG26++eZgrwYA0MZ1bI6FDho0SJs3b/52JR2bZTUAgDasWZKhY8eOio+Pb45FAwDaiWa5BnTgwAElJiaqb9++euihh1RaWtrkvDU1NfJ6vX4DAKD9C3oApaSkaPXq1dq4caNycnJUUlKiMWPG6PTp043On52dLbfb7RuSkpKC3RIAoBUKMcaY5lxBRUWFevfurV/96ld69NFHG0yvqalRTU2N77XX6yWEAKAd8Hg8ioqKanJ6s98dEB0drQEDBujgwYONTne5XHK5XM3dBgCglWn2vwM6c+aMiouLlZCQ0NyrAgC0IUEPoKefflr5+fk6dOiQPv30U917770KDQ3VtGnTgr0qAEAbFvRTcEePHtW0adN06tQpxcTEaPTo0dqxY4diYmKCvSoAQBvW7DchOOX1euV2u223gWaSnp7uuOaDDz5wXHP33Xc7rpGkDRs2BFQHaeXKlY5rZs2a5bhm8ODBjmu++OILxzW4dle6CYFnwQEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFc3+hXTAdz3//POOawJ5Xu4//MM/OK6ReBjptXjooYcc19TX1zdDJ2grOAICAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFTwNGwHr0MH57y9JSUnN0ElDH3zwQYusB0DgOAICAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACt4GCkCds899ziuSUxMbIZOEGyjRo1yXBMeHt4MnaA94wgIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKzgYaQIWP/+/VtkPR999JHjmn379jVDJ9ePAQMGOK7p2JGPEzjDERAAwAoCCABgheMA2rZtm+666y4lJiYqJCRE77//vt90Y4wWLFighIQEde7cWampqTpw4ECw+gUAtBOOA6iyslJDhw7VihUrGp2+dOlSvf7661q5cqV27typiIgIpaWlqbq6+pqbBQC0H46vGqanpys9Pb3RacYYLVu2TD//+c9935b51ltvKS4uTu+//76mTp16bd0CANqNoF4DKikpUVlZmVJTU33j3G63UlJSVFBQ0GhNTU2NvF6v3wAAaP+CGkBlZWWSpLi4OL/xcXFxvmmXys7Oltvt9g1JSUnBbAkA0EpZvwsuKytLHo/HNxw5csR2SwCAFhDUAIqPj5cklZeX+40vLy/3TbuUy+VSVFSU3wAAaP+CGkDJycmKj4/Xli1bfOO8Xq927typkSNHBnNVAIA2zvFdcGfOnNHBgwd9r0tKSrRnzx5169ZNvXr10rx587RkyRLddNNNSk5O1vPPP6/ExERNnjw5mH0DANo4xwG0a9cu3X777b7X8+fPlyRlZGRo9erVeuaZZ1RZWamZM2eqoqJCo0eP1saNG9WpU6fgdQ0AaPNCjDHGdhPf5fV65Xa7bbdxXRk4cGBAdXv27HFcExIS4rhm9OjRjmt27drluAbfWr58ueOa2bNnO64pLS11XPPDH/7Qcc2l16XRMjwez2Wv61u/Cw4AcH0igAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACsdfx4D2JywsLKC68PBwxzVVVVWOa1r7k60jIiIc19x0000tsp5//dd/dVwjSXfffXdAdU4VFxc7ruHJ1u0HR0AAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUPI0WLCg0NdVzTt29fxzV///vfHdcEasGCBY5rnn766WbopO0ZNmyY45o+ffo4rjl06JDjGjQ/joAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoeRgqdPHkyoLrDhw87rundu7fjmsLCQsc11dXVjmsCFRkZ2SLrMcY4rjl37lxA6woLC3NcE0h/gTycNiIiwnENWieOgAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACh5GCpWXlwdU95vf/MZxzZIlSxzXdO3atUVqAlVZWem45m9/+5vjmtdff91xzT333OO4RpJ+/OMfO645duyY45oRI0Y4rmnJB82ieXEEBACwggACAFjhOIC2bdumu+66S4mJiQoJCdH777/vN3369OkKCQnxGyZOnBisfgEA7YTjAKqsrNTQoUO1YsWKJueZOHGijh8/7hveeeeda2oSAND+OL4JIT09Xenp6Zedx+VyKT4+PuCmAADtX7NcA8rLy1NsbKwGDhyoxx9/XKdOnWpy3pqaGnm9Xr8BAND+BT2AJk6cqLfeektbtmzRyy+/rPz8fKWnp+v8+fONzp+dnS232+0bkpKSgt0SAKAVCvrfAU2dOtX371tuuUVDhgxRv379lJeXp/HjxzeYPysrS/Pnz/e99nq9hBAAXAea/Tbsvn37qkePHjp48GCj010ul6KiovwGAED71+wBdPToUZ06dUoJCQnNvSoAQBvi+BTcmTNn/I5mSkpKtGfPHnXr1k3dunXT4sWLNWXKFMXHx6u4uFjPPPOM+vfvr7S0tKA2DgBo2xwH0K5du3T77bf7Xl+8fpORkaGcnBzt3btXv/3tb1VRUaHExERNmDBBv/jFL+RyuYLXNQCgzXMcQOPGjZMxpsnpH3300TU1hLZj06ZNjmvuv/9+xzW33nqr45pA1dXVOa7JyMhwXJObm+u4plOnTo5rnnjiCcc1gSoqKnJcU1ZW1gydoK3gWXAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwIsRc7tHWFni9XrndbtttoJkE8kTnQGoCFch/B4/H0wydNNS7d2/HNX//+98DWld1dbXjmvT0dMc127Ztc1yDtsPj8Vz2W645AgIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKzrabgDXl0AechlITXv06KOPtti63nnnHcc1PFgUTnEEBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABW8DBSoI2YOnWq7RaAoOIICABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCs4GGkQBvRo0cPxzW1tbUBrWv58uUB1QFOcAQEALCCAAIAWOEogLKzs3XbbbcpMjJSsbGxmjx5soqKivzmqa6uVmZmprp3766uXbtqypQpKi8vD2rTAIC2z1EA5efnKzMzUzt27NCmTZtUV1enCRMmqLKy0jfPU089pT/96U9at26d8vPzdezYMd13331BbxwA0LaFGGNMoMUnT55UbGys8vPzNXbsWHk8HsXExGjt2rW6//77JUn79+/X9773PRUUFOiHP/zhFZfp9XrldrsDbQlot/7v//7PcU3nzp0DWtfV/F+91N/+9reA1oX2y+PxKCoqqsnp13QNyOPxSJK6desmSdq9e7fq6uqUmprqm+fmm29Wr169VFBQ0Ogyampq5PV6/QYAQPsXcADV19dr3rx5GjVqlAYPHixJKisrU3h4uKKjo/3mjYuLU1lZWaPLyc7Oltvt9g1JSUmBtgQAaEMCDqDMzEzt27dP77777jU1kJWVJY/H4xuOHDlyTcsDALQNAf0h6pw5c7RhwwZt27ZNPXv29I2Pj49XbW2tKioq/I6CysvLFR8f3+iyXC6XXC5XIG0AANowR0dAxhjNmTNHubm5+vjjj5WcnOw3ffjw4QoLC9OWLVt844qKilRaWqqRI0cGp2MAQLvg6AgoMzNTa9eu1fr16xUZGem7ruN2u9W5c2e53W49+uijmj9/vrp166aoqCjNnTtXI0eODOiuGgBA++UogHJyciRJ48aN8xu/atUqTZ8+XZL0H//xH+rQoYOmTJmimpoapaWl6de//nVQmgUAtB/X9HdAzYG/A8L1oKlropfzP//zP45r9uzZ47hGksaOHRtQHfBdzfp3QAAABIoAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArAvpGVADXZt68eY5rIiIiHNfs27fPcQ3QUjgCAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAAreBgpYMFXX33VIuvJy8trkfUAgeAICABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCs4GGkgAVbtmxxXFNVVdUMnQD2cAQEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFbwMFLAgqNHjzqu2b17dzN0AtjDERAAwAoCCABghaMAys7O1m233abIyEjFxsZq8uTJKioq8ptn3LhxCgkJ8Rsee+yxoDYNAGj7HAVQfn6+MjMztWPHDm3atEl1dXWaMGGCKisr/eabMWOGjh8/7huWLl0a1KYBAG2fo5sQNm7c6Pd69erVio2N1e7duzV27Fjf+C5duig+Pj44HQIA2qVrugbk8XgkSd26dfMbv2bNGvXo0UODBw9WVlbWZb9KuKamRl6v128AALR/Ad+GXV9fr3nz5mnUqFEaPHiwb/yDDz6o3r17KzExUXv37tWzzz6roqIi/fGPf2x0OdnZ2Vq8eHGgbQAA2qiAAygzM1P79u3T9u3b/cbPnDnT9+9bbrlFCQkJGj9+vIqLi9WvX78Gy8nKytL8+fN9r71er5KSkgJtCwDQRgQUQHPmzNGGDRu0bds29ezZ87LzpqSkSJIOHjzYaAC5XC65XK5A2gAAtGGOAsgYo7lz5yo3N1d5eXlKTk6+Ys2ePXskSQkJCQE1CABonxwFUGZmptauXav169crMjJSZWVlkiS3263OnTuruLhYa9eu1aRJk9S9e3ft3btXTz31lMaOHashQ4Y0yw8AAGibHAVQTk6OpAt/bPpdq1at0vTp0xUeHq7Nmzdr2bJlqqysVFJSkqZMmaKf//znQWsYANA+OD4FdzlJSUnKz8+/poYAANeHEHOlVGlhXq9XbrfbdhsAgGvk8XgUFRXV5HQeRgoAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGBFqwsgY4ztFgAAQXClz/NWF0CnT5+23QIAIAiu9HkeYlrZIUd9fb2OHTumyMhIhYSE+E3zer1KSkrSkSNHFBUVZalD+9gOF7AdLmA7XMB2uKA1bAdjjE6fPq3ExER16ND0cU7HFuzpqnTo0EE9e/a87DxRUVHX9Q52EdvhArbDBWyHC9gOF9jeDm63+4rztLpTcACA6wMBBACwok0FkMvl0sKFC+VyuWy3YhXb4QK2wwVshwvYDhe0pe3Q6m5CAABcH9rUERAAoP0ggAAAVhBAAAArCCAAgBUEEADAijYTQCtWrFCfPn3UqVMnpaSk6LPPPrPdUotbtGiRQkJC/Iabb77ZdlvNbtu2bbrrrruUmJiokJAQvf/++37TjTFasGCBEhIS1LlzZ6WmpurAgQN2mm1GV9oO06dPb7B/TJw40U6zzSQ7O1u33XabIiMjFRsbq8mTJ6uoqMhvnurqamVmZqp79+7q2rWrpkyZovLycksdN4+r2Q7jxo1rsD889thjljpuXJsIoPfee0/z58/XwoUL9fnnn2vo0KFKS0vTiRMnbLfW4gYNGqTjx4/7hu3bt9tuqdlVVlZq6NChWrFiRaPTly5dqtdff10rV67Uzp07FRERobS0NFVXV7dwp83rSttBkiZOnOi3f7zzzjst2GHzy8/PV2Zmpnbs2KFNmzaprq5OEyZMUGVlpW+ep556Sn/605+0bt065efn69ixY7rvvvssdh18V7MdJGnGjBl++8PSpUstddwE0waMGDHCZGZm+l6fP3/eJCYmmuzsbItdtbyFCxeaoUOH2m7DKkkmNzfX97q+vt7Ex8ebV155xTeuoqLCuFwu884771josGVcuh2MMSYjI8Pcc889Vvqx5cSJE0aSyc/PN8ZceO/DwsLMunXrfPN89dVXRpIpKCiw1Wazu3Q7GGPMj370I/Pkk0/aa+oqtPojoNraWu3evVupqam+cR06dFBqaqoKCgosdmbHgQMHlJiYqL59++qhhx5SaWmp7ZasKikpUVlZmd/+4Xa7lZKScl3uH3l5eYqNjdXAgQP1+OOP69SpU7ZbalYej0eS1K1bN0nS7t27VVdX57c/3HzzzerVq1e73h8u3Q4XrVmzRj169NDgwYOVlZWlqqoqG+01qdU9DftSX3/9tc6fP6+4uDi/8XFxcdq/f7+lruxISUnR6tWrNXDgQB0/flyLFy/WmDFjtG/fPkVGRtpuz4qysjJJanT/uDjtejFx4kTdd999Sk5OVnFxsZ577jmlp6eroKBAoaGhttsLuvr6es2bN0+jRo3S4MGDJV3YH8LDwxUdHe03b3veHxrbDpL04IMPqnfv3kpMTNTevXv17LPPqqioSH/84x8tduuv1QcQvpWenu7795AhQ5SSkqLevXvr97//vR599FGLnaE1mDp1qu/ft9xyi4YMGaJ+/fopLy9P48ePt9hZ88jMzNS+ffuui+ugl9PUdpg5c6bv37fccosSEhI0fvx4FRcXq1+/fi3dZqNa/Sm4Hj16KDQ0tMFdLOXl5YqPj7fUVesQHR2tAQMG6ODBg7ZbsebiPsD+0VDfvn3Vo0ePdrl/zJkzRxs2bNDWrVv9vj8sPj5etbW1qqio8Ju/ve4PTW2HxqSkpEhSq9ofWn0AhYeHa/jw4dqyZYtvXH19vbZs2aKRI0da7My+M2fOqLi4WAkJCbZbsSY5OVnx8fF++4fX69XOnTuv+/3j6NGjOnXqVLvaP4wxmjNnjnJzc/Xxxx8rOTnZb/rw4cMVFhbmtz8UFRWptLS0Xe0PV9oOjdmzZ48kta79wfZdEFfj3XffNS6Xy6xevdp8+eWXZubMmSY6OtqUlZXZbq1F/fu//7vJy8szJSUl5pNPPjGpqammR48e5sSJE7Zba1anT582hYWFprCw0Egyv/rVr0xhYaE5fPiwMcaYl156yURHR5v169ebvXv3mnvuucckJyebs2fPWu48uC63HU6fPm2efvppU1BQYEpKSszmzZvNsGHDzE033WSqq6tttx40jz/+uHG73SYvL88cP37cN1RVVfnmeeyxx0yvXr3Mxx9/bHbt2mVGjhxpRo4cabHr4LvSdjh48KB54YUXzK5du0xJSYlZv3696du3rxk7dqzlzv21iQAyxpjly5ebXr16mfDwcDNixAizY8cO2y21uAceeMAkJCSY8PBwc+ONN5oHHnjAHDx40HZbzW7r1q1GUoMhIyPDGHPhVuznn3/exMXFGZfLZcaPH2+KiorsNt0MLrcdqqqqzIQJE0xMTIwJCwszvXv3NjNmzGh3v6Q19vNLMqtWrfLNc/bsWTN79mxzww03mC5duph7773XHD9+3F7TzeBK26G0tNSMHTvWdOvWzbhcLtO/f3/z05/+1Hg8HruNX4LvAwIAWNHqrwEBANonAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACw4v8BTdBSo6oSS9AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}